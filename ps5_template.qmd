---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): luyao Guo , luyao1
    - Partner 2 (name and cnet ID): Ruyu Zhang, Ruyu Zhang
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


```{python}
import requests
from bs4 import BeautifulSoup
from datetime import datetime
```

## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
# Set up the URL and retrieve page content
page_url = 'https://oig.hhs.gov/fraud/enforcement/'
page_response = requests.get(page_url)
soup = BeautifulSoup(page_response.text, 'html.parser')

# Initialize lists to store the extracted data
category_data = []
title_data = []
date_data = []
url_links = []

# Locate the main content sections for each enforcement action
entries = soup.find_all(
    'li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')

# Loop through each entry to collect title, date, category, and link
for entry in entries:
    # Extract title
    title_block = entry.find('h2', class_='usa-card__heading')
    title = title_block.get_text(
        strip=True) if title_block else 'Title Not Available'

    # Extract link
    link_tag = title_block.find('a') if title_block else None
    link_url = f"https://oig.hhs.gov{link_tag['href']
                                     }" if link_tag else 'Link Not Available'

    # Extract date
    date_block = entry.find('span', class_='text-base-dark padding-right-105')
    date = date_block.get_text(
        strip=True) if date_block else 'Date Not Available'

    # Extract category
    category_block = entry.find(
        'li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
    category = category_block.get_text(
        strip=True) if category_block else 'Category Not Available'

    # Append data to respective lists
    category_data.append(category)
    title_data.append(title)
    date_data.append(date)
    url_links.append(link_url)

# Store data in a DataFrame
enforcement_data = pd.DataFrame({
    'Enforcement Category': category_data,
    'Enforcement Title': title_data,
    'Enforcement Date': date_data,
    'Enforcement Link': url_links
})

# Display the first few rows of the DataFrame
print("Scraped DataFrame:")
print(enforcement_data.head())

```

  


### 2. Crawling (PARTNER 1)

```{python}
# Initialize list to store agency names
agency_names = []

# Function to extract agency name from each individual page
def get_agency_name(link):
    response = requests.get(link)
    page_soup = BeautifulSoup(response.text, 'html.parser')
    
    # Default message if agency information is not found
    agency_info = 'Agency Not Found'
    
    try:
        # Locate the "Agency:" span
        agency_label = page_soup.find('span', class_='padding-right-2 text-base', string="Agency:")
        if agency_label:
            # If found, get the next sibling text (which is likely the agency name)
            agency_info = agency_label.find_next_sibling(text=True).strip()
    
    except Exception as e:
        print(f"Error occurred for link {link}: {e}")
    
    return agency_info

# Crawl each link in the DataFrame and get the agency name
for link in enforcement_data['Enforcement Link']:
    agency_name = get_agency_name(link)
    agency_names.append(agency_name)
    time.sleep(1)  # Add a delay to be polite to the server

# Add the agency names to the DataFrame
enforcement_data['Agency Name'] = agency_names

# Display updated DataFrame
print("Updated DataFrame with Agency Information:")
print(enforcement_data.head())

```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

Input Validation:
Check if the input year is 2013 or later.
If the year is less than 2013, print a message and exit the function.

Initialization:
Define the base URL for the enforcement actions page.
Initialize lists to store data for titles, dates, categories, links, and agencies.
Set the initial page number to 1.
Initialize a variable keep_scraping as True to control the main loop.

Main Loop (While Loop):
Loop Definition: A while loop that continues as long as keep_scraping is True.
Purpose: This loop will go through each page of the enforcement actions, scraping data until there are no more entries or until the specified date range is exceeded.
Inside the While Loop:
  Construct Page URL:
    Append the page number to the base URL to form the full URL for the current page.
  Send HTTP Request:
    Use requests.get() to retrieve the page content.
    Parse the HTML content with BeautifulSoup.
  Locate Enforcement Entries:
    Search for the main content section containing all enforcement actions on the page.
    If no entries are found, set keep_scraping to False and break out of the loop.

Inner Loop (For Loop):
Loop Definition: A for loop that iterates over each enforcement action found in the current page's entries.
Purpose: This loop extracts the details (title, date, category, link, and agency name) for each enforcement action.
Inside the For Loop:
  Extract Date:
    Retrieve and parse the date of the enforcement action.
    If the date is older than the specified starting month and year, set keep_scraping to False and break out of the loop.
  Extract Title, Link, and Category:
    Retrieve and store the title, link, and category of the enforcement action.
  Extract Agency Name:
    Call the get_agency_name function with the link to fetch the agency name from the detailed page.
  Append Data to Lists:
    Add the extracted data to the corresponding lists (titles, dates, categories, links, agencies).
End of Inner For Loop:
After processing each entry, increment the page number by 1 to move to the next page and add a 1-second delay to prevent server overload.

Create DataFrame:
Once the loop completes, create a DataFrame from the lists containing enforcement data.

Save Data to CSV:
Save the DataFrame as a CSV file named based on the input year and month.
Print a confirmation message with the file name.

Function Call Example:
Call scrape_enforcement_actions with specific year and month values, such as scrape_enforcement_actions(2023, 1).


* b. Create Dynamic Scraper (PARTNER 2)

```{python}
def scrape_enforcement_actions(year, month):
    if year < 2013:
        print("The year should be restricted to 2013 or later.")
        return

    # Set the URL and initialize lists to store data
    base_url = 'https://oig.hhs.gov/fraud/enforcement/'
    titles, dates, categories, links, agencies = [], [], [], [], []

    page = 1
    keep_scraping = True

    while keep_scraping:
        # Construct the URL with pagination
        url = f"{base_url}?page={page}"
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')

        # Locate the main content sections for each enforcement action
        entries = soup.find_all(
            'li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')

        if not entries:  # Stop if no entries are found on the page
            break

        for entry in entries:
            # Extract the date and stop if we exceed the specified date range
            date_block = entry.find(
                'span', class_='text-base-dark padding-right-105')
            date_text = date_block.get_text(strip=True)
            entry_date = datetime.strptime(date_text, '%B %d, %Y')
            if entry_date.year < year or (entry_date.year == year and entry_date.month < month):
                keep_scraping = False
                break

            # Extract other details if within the date range
            title_block = entry.find('h2', class_='usa-card__heading')
            title = title_block.get_text(strip=True)

            link_tag = title_block.find('a')
            link_url = f"https://oig.hhs.gov{link_tag['href']}"

            category_block = entry.find(
                'li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
            category = category_block.get_text(strip=True)

            # Get agency name from the linked page
            agency_name = get_agency_name(link_url)

            # Append data to lists
            dates.append(date_text)
            titles.append(title)
            links.append(link_url)
            categories.append(category)
            agencies.append(agency_name)

        # Increment page number and wait before the next request
        page += 1
        time.sleep(1)

    # Create and save DataFrame
    df_enforcement = pd.DataFrame({
        'Enforcement Date': dates,
        'Enforcement Title': titles,
        'Enforcement Category': categories,
        'Enforcement Link': links,
        'Agency Name': agencies
    })

    # Save to CSV
    file_name = f"enforcement_actions_{year}_{month}.csv"
    df_enforcement.to_csv(file_name, index=False)
    print(f"Data saved to {file_name}")


scrape_enforcement_actions(2023, 1)
```
There are 1534 enforcement actions.


* c. Test Partner's Code (PARTNER 1)

```{python}
scrape_enforcement_actions(2021, 1)
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}
df_enforcement_2021 = pd.read_csv('enforcement_actions_2023_1.csv')
df_enforcement_2021['Enforcement Date'] = pd.to_datetime(df_enforcement_2021['Enforcement Date'], errors='coerce')
df_enforcement_2021['year_month'] = df_enforcement_2021['Enforcement Date'].dt.to_period('M')
df_enforcement_2021_monthly = df_enforcement_2021.groupby(['year_month']).size().reset_index(name='num_enforcement')
df_enforcement_2021_monthly['year_month'] = df_enforcement_2021_monthly['year_month'].dt.to_timestamp()

line_chart_2021=alt.Chart(df_enforcement_2021_monthly, title="The number of enforcement actions over time").mark_line().encode(
    alt.X("yearmonth(year_month):O", title="Time")  
        .axis(
            format="%Y",
            labelAngle=0, 
        ), 
    alt.Y('num_enforcement:Q', title='Number of enforcement actions'),
    tooltip=['year_month', 'num_enforcement']
).properties(
    width=600,
    height=300
)

line_chart_2021

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}
import geopandas as gpd

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```